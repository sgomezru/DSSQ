{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from monai.metrics import DiceMetric\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "REPO_PATH = '/workspace/repositories/DSSQ/src'\n",
    "OUT_PATH = '/workspace/out'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "sys.path.append(REPO_PATH)\n",
    "\n",
    "from adapters import DimReductAdapter, DimReductModuleWrapper\n",
    "from data_utils import get_eval_data\n",
    "from models import get_unet\n",
    "from utils import epoch_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set mode\n",
    "MODE = 'eval'\n",
    "LOG = False\n",
    "ITERATION = 0\n",
    "DATA_KEY = 'prostate'\n",
    "LOAD_ONLY_PRESENT = True\n",
    "VALIDATION = True\n",
    "EXTRA_DESCRIPTION = '_base'\n",
    "N_DIMS = [2, 4]\n",
    "\n",
    "cfg = OmegaConf.load(f'{REPO_PATH}/configs/conf.yaml')\n",
    "OmegaConf.update(cfg, 'run.iteration', ITERATION)\n",
    "OmegaConf.update(cfg, 'run.data_key', DATA_KEY)\n",
    "\n",
    "unet_name = 'monai-64-4-4'\n",
    "args = unet_name.split('-')\n",
    "cfg.wandb.log = LOG\n",
    "cfg.unet[DATA_KEY].pre = unet_name\n",
    "cfg.unet[DATA_KEY].arch = args[0]\n",
    "cfg.unet[DATA_KEY].n_filters_init = None if unet_name == 'swinunetr' else int(args[1])\n",
    "cfg.unet[DATA_KEY].training.load_only_present = LOAD_ONLY_PRESENT\n",
    "cfg.unet[DATA_KEY].training.validation = VALIDATION\n",
    "cfg.unet[DATA_KEY].training.batch_size = 32\n",
    "cfg.wandb.project = f'{DATA_KEY}_{unet_name}_{ITERATION}{EXTRA_DESCRIPTION}'\n",
    "if args[0] == 'monai':\n",
    "    cfg.unet[DATA_KEY].depth = int(args[2])\n",
    "    cfg.unet[DATA_KEY].num_res_units = int(args[3])\n",
    "# Set cfg.format to \"numpy\" for evaluation,\n",
    "# otherwise don't modify it or set to \"torch\" for training\n",
    "if MODE == 'eval': cfg.format = 'numpy'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "layer_names = [f'model.{\"1.submodule.\" * i}0.conv' for i in range(cfg.unet[DATA_KEY].depth)]\n",
    "train_vendors = ['siemens']\n",
    "test_vendors = ['siemens', 'philips', 'ge']\n",
    "dim_red_modes = ['IPCA', 'PCA']\n",
    "th_values = [0.8, 0.85, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_set(cfg, model, dataset):\n",
    "    if cfg.run.data_key == 'prostate':\n",
    "        dataloader = DataLoader(dataset, batch_size=cfg.unet.prostate.training.batch_size,\n",
    "                                shuffle=False, drop_last=False)\n",
    "        eval_metrics = {\n",
    "            \"Dice Score\": DiceMetric(\n",
    "                ignore_empty=True,\n",
    "                include_background=False\n",
    "            )\n",
    "        }\n",
    "        metrics = eval_pmri_set(model=model, dataloader=dataloader, eval_metrics=eval_metrics)\n",
    "    else:\n",
    "        raise ValueError( f'Invalid data key. No config for dataset named {cfg.run.data_key}')\n",
    "    return metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_pmri_set(model, dataloader, eval_metrics):\n",
    "    model.eval()\n",
    "    epoch_metrics = {key: [] for key in eval_metrics.keys()}\n",
    "    batch_sizes = []\n",
    "    for batch in dataloader:\n",
    "        input_ = batch['input']\n",
    "        target = batch['target']\n",
    "        batch_sizes.append(input_.shape[0])\n",
    "        out = model(input_.cuda()).detach().cpu()\n",
    "        out = torch.argmax(out, dim=1).unsqueeze(1)\n",
    "        for key, metric in eval_metrics.items():\n",
    "            computed_metric = metric(out, target).detach().mean().cpu()\n",
    "            epoch_metrics[key].append(computed_metric)\n",
    "    for key, epoch_scores in epoch_metrics.items():\n",
    "        epoch_metrics[key] = epoch_average(epoch_scores, batch_sizes)\n",
    "    return epoch_metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_pmri_ood(model, dataset):\n",
    "    model.eval()\n",
    "    dice_scores = []\n",
    "    dm = DiceMetric(ignore_empty=True, include_background=False)\n",
    "    dataloader = DataLoader(dataset, batch_size=cfg.unet.prostate.training.batch_size,\n",
    "                            shuffle=False, drop_last=False)\n",
    "    ret = {}\n",
    "    for batch in dataloader:\n",
    "        input_ = batch['input']\n",
    "        target = batch['target']\n",
    "        out = model(input_.cuda())\n",
    "        for key in out:\n",
    "            out[key] = out[key].detach().cpu()\n",
    "        for adapter in model.adapters:\n",
    "            ret[adapter.swivel] = ret.get(adapter.swivel, []) + [out[f'{adapter.swivel}_ood']]\n",
    "        seg_mask = torch.argmax(out['seg'], dim=1).unsqueeze(1)\n",
    "        batch_dices = dm(seg_mask, target).detach().cpu()\n",
    "        dice_scores.append(batch_dices)\n",
    "\n",
    "    dice_scores = torch.cat(dice_scores, dim=0)\n",
    "    ret['dice_scores'] = dice_scores.detach().cpu()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_thresh = True\n",
    "metrics = {}\n",
    "thresh = {}\n",
    "outputs = {}\n",
    "for vendor in train_vendors:\n",
    "    cfg.unet[DATA_KEY].training.subset = 'training'\n",
    "    cfg.unet[DATA_KEY].training.vendor = vendor\n",
    "    data = get_eval_data(train_set=False, val_set=False, eval_set=True, cfg=cfg)\n",
    "    for dim_red_mode in dim_red_modes:\n",
    "        for n_dims in N_DIMS:\n",
    "            op_mode = f'{n_dims}_{dim_red_mode}' \n",
    "            metrics[op_mode] = metrics.get(op_mode, {})\n",
    "            thresh[op_mode] = thresh.get(op_mode, {})\n",
    "            outputs[op_mode] = outputs.get(op_mode, {})\n",
    "            print(f'Running for vendor {vendor}, {dim_red_mode} mode of {n_dims} dims')\n",
    "            adapters = [DimReductAdapter(swivel, n_dims, cfg.unet[DATA_KEY].training.batch_size,\n",
    "                                            mode=dim_red_mode, pre_fit=True, fit_gaussian=False,\n",
    "                                            project=cfg.wandb.project) for swivel in layer_names]\n",
    "            adapters = nn.ModuleList(adapters)\n",
    "            unet, state_dict = get_unet(cfg, update_cfg_with_swivels=False, return_state_dict=True)\n",
    "            unet_adapted = DimReductModuleWrapper(model=unet, adapters=adapters)\n",
    "            unet_adapted.to(device);\n",
    "            unet_adapted.eval();\n",
    "            metrics[op_mode][f'{vendor}_train'] = eval_set(cfg, unet_adapted, data['eval'])\n",
    "            # OOD downstream task\n",
    "            unet_adapted.set_downstream_ood(True)\n",
    "            # Threshold compute\n",
    "            for th in th_values:\n",
    "                thresh[op_mode][th] = unet_adapted.compute_thresholds(th)\n",
    "            # Not in the same loop, cause didn't want to stack computed distances for\n",
    "            # previous threshold computation\n",
    "            for th in th_values:\n",
    "                unet_adapted.set_thresholds(thresh[op_mode][th])\n",
    "                outputs[op_mode][th] = eval_pmri_ood(unet_adapted, data['eval'])\n",
    "            \n",
    "\n",
    "# with open(f'{OUT_PATH}/ood_thresholds/thresh.json', 'w') as f:\n",
    "#     json.dump(thresh, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1998)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tmp_arr = torch.cat(outputs['2_PCA'][0.8]['model.0.conv'])\n",
    "# total = len(tmp_arr)\n",
    "# ood = torch.sum(tmp_arr)\n",
    "# ood / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model.0.conv', 'model.1.submodule.0.conv', 'model.1.submodule.1.submodule.0.conv', 'model.1.submodule.1.submodule.1.submodule.0.conv', 'dice_scores'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['2_IPCA'][0.9].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vendor in test_vendors:\n",
    "    cfg.unet[DATA_KEY].training.subset = 'validation' if vendor in train_vendors else False \n",
    "    cfg.unet[DATA_KEY].training.vendor = vendor\n",
    "    data = get_eval_data(train_set=False, val_set=False, eval_set=True, cfg=cfg)\n",
    "    for dim_red_mode in dim_red_modes:\n",
    "        for n_dims in N_DIMS:\n",
    "            op_mode = f'{n_dims}_{dim_red_mode}' \n",
    "            metrics[op_mode] = metrics.get(op_mode, {})\n",
    "            outputs[op_mode] = outputs.get(op_mode, {})\n",
    "            print(f'Running for vendor {vendor}, {dim_red_mode} mode of {n_dims} dims')\n",
    "            adapters = [DimReductAdapter(swivel, n_dims, cfg.unet[DATA_KEY].training.batch_size,\n",
    "                                            mode=dim_red_mode, pre_fit=True, fit_gaussian=False,\n",
    "                                            project=cfg.wandb.project) for swivel in layer_names]\n",
    "            adapters = nn.ModuleList(adapters)\n",
    "            unet, state_dict = get_unet(cfg, update_cfg_with_swivels=False, return_state_dict=True)\n",
    "            unet_adapted = DimReductModuleWrapper(model=unet, adapters=adapters, downstream_ood=True)\n",
    "            unet_adapted.to(device);\n",
    "            unet_adapted.eval();\n",
    "            # Both dice and downstream ood eval\n",
    "            for th in th_values:\n",
    "                unet_adapted.set_thresholds(thresh[op_mode][th])\n",
    "                outputs[op_mode][th] = eval_pmri_ood(unet_adapted, data['eval'])\n",
    "            \n",
    "            metrics[op_mode][f'{vendor}_test'] = eval_set(cfg, unet_adapted, data['eval'])\n",
    "\n",
    "# with open(f'{OUT_PATH}/ood_thresholds/thresh.json', 'w') as f:\n",
    "#     json.dump(thresh, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
